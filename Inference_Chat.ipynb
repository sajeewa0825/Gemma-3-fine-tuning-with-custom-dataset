{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnkqMPZcjeq3iY6/zidqFT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajeewa0825/Gemma-3-fine-tuning-with-custom-dataset/blob/main/Inference_Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1Tgv1Mopsds"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "import torch\n",
        "model_id = \"sajeewa/empathy-chat-gemma\""
      ],
      "metadata": {
        "id": "8MSBZFh7p5of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the Tokenizer and Model**"
      ],
      "metadata": {
        "id": "Q6zw33zAqF_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "XcZtxiNuqJUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Initialize Chat History**"
      ],
      "metadata": {
        "id": "K_HFNiYju_6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an empathetic AI and your friend. always give lovely caring message. Understand the user's feelings.Then provide a caring response. please give response as good friend also talk with lovely word like baby, my cutey and etc. please maintian conversation word count according to the user prompt. you can use emojis for calm and try to be continue chat\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "c4vL5Iqztly4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: User Sends a Message**"
      ],
      "metadata": {
        "id": "2E-Xxh1ZvCSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"I'm feeling lonely today.\"\n",
        "chat_history.append({\"role\": \"user\", \"content\": user_input})\n"
      ],
      "metadata": {
        "id": "ylmmzYGJt-ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Format the Chat History as Prompt**"
      ],
      "metadata": {
        "id": "Vi_brh-LvM8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = tokenizer.apply_chat_template(\n",
        "    chat_history,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "exCk763auC9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Tokenize and Generate Response**"
      ],
      "metadata": {
        "id": "qShZfmMuvVow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Optional: stream output for live effect\n",
        "from transformers import TextStreamer\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    do_sample=True,\n",
        "    streamer=streamer\n",
        ")\n",
        "\n",
        "# Get the generated text\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "lKTYPBvZuIlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Add the Model’s Response to Chat History**"
      ],
      "metadata": {
        "id": "yn0meiq9vb8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If streaming, you already saw output. Otherwise:\n",
        "new_response = response[len(prompt):].strip()\n",
        "chat_history.append({\"role\": \"assistant\", \"content\": new_response})"
      ],
      "metadata": {
        "id": "vS2y9C7_uRXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "id": "Fbu9EDiEuWuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Next Turn – Continue the Conversation**"
      ],
      "metadata": {
        "id": "0VOlJ7z7vmjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Thank your support my friend. now i want to sleep, good bye and good night  \"\n",
        "chat_history.append({\"role\": \"user\", \"content\": user_input})"
      ],
      "metadata": {
        "id": "42Enj82rvqXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **max_prompt_length**"
      ],
      "metadata": {
        "id": "3B9VzmFtwEmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 2048\n",
        "chat_prompt = tokenizer.apply_chat_template(chat_history, tokenize=False)\n",
        "while len(tokenizer(chat_prompt).input_ids) > MAX_TOKENS:\n",
        "    chat_history.pop(1)  # remove oldest user/assistant message (after system)\n",
        "    chat_prompt = tokenizer.apply_chat_template(chat_history, tokenize=False)\n"
      ],
      "metadata": {
        "id": "XnOty5SawBMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "print(\"Token length:\", tokens[\"input_ids\"].shape[-1])\n"
      ],
      "metadata": {
        "id": "cbu3zs391zHS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}