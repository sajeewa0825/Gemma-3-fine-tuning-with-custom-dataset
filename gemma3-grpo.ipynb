{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11371230,"sourceType":"datasetVersion","datasetId":7118605}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:08:47.935771Z","iopub.execute_input":"2025-04-13T15:08:47.936007Z","iopub.status.idle":"2025-04-13T15:09:21.893346Z","shell.execute_reply.started":"2025-04-13T15:08:47.935991Z","shell.execute_reply":"2025-04-13T15:09:21.892485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:11:01.724941Z","iopub.execute_input":"2025-04-13T15:11:01.725586Z","iopub.status.idle":"2025-04-13T15:13:37.123709Z","shell.execute_reply.started":"2025-04-13T15:11:01.725556Z","shell.execute_reply":"2025-04-13T15:13:37.122980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastModel\nimport torch\nmax_seq_length = 1024\n\nfourbit_models = [\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n\n    # Other popular models!\n    \"unsloth/Llama-3.1-8B\",\n    \"unsloth/Llama-3.2-3B\",\n    \"unsloth/Llama-3.3-70B\",\n    \"unsloth/mistral-7b-instruct-v0.3\",\n    \"unsloth/Phi-4\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gemma-3-1b-it\",\n    max_seq_length = max_seq_length, # Choose any for long context!\n    load_in_4bit = False,  # 4 bit quantization to reduce memory\n    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:14:44.403468Z","iopub.execute_input":"2025-04-13T15:14:44.404230Z","iopub.status.idle":"2025-04-13T15:15:32.977587Z","shell.execute_reply.started":"2025-04-13T15:14:44.404200Z","shell.execute_reply":"2025-04-13T15:15:32.977016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastModel.get_peft_model(\n    model,\n    finetune_vision_layers     = False, # Turn off for just text!\n    finetune_language_layers   = True,  # Should leave on!\n    finetune_attention_modules = True,  # Attention good for GRPO\n    finetune_mlp_modules       = True,  # SHould leave on always!\n\n    r = 8,           # Larger = higher accuracy, but might overfit\n    lora_alpha = 8,  # Recommended alpha == r at least\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:16:20.543968Z","iopub.execute_input":"2025-04-13T15:16:20.544540Z","iopub.status.idle":"2025-04-13T15:16:24.982810Z","shell.execute_reply.started":"2025-04-13T15:16:20.544516Z","shell.execute_reply":"2025-04-13T15:16:24.981998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/preprocess-empathy-chat/preprocess_empathy_data.csv\")\ndf = df.rename(columns={\"utterance\": \"response\"})\ndf = df.dropna(subset=[\"prompt\", \"response\"])  # Remove empty entries\ndf[\"prompt\"] = df[\"prompt\"].astype(str)       # Ensure string type\ndf[\"response\"] = df[\"response\"].astype(str)    # Ensure string type\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:19:05.654285Z","iopub.execute_input":"2025-04-13T15:19:05.654812Z","iopub.status.idle":"2025-04-13T15:19:05.832262Z","shell.execute_reply.started":"2025-04-13T15:19:05.654788Z","shell.execute_reply":"2025-04-13T15:19:05.831681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reasoning_start = \"<start_working_out>\"\nreasoning_end   = \"<end_working_out>\"\nsolution_start  = \"<SOLUTION>\"\nsolution_end    = \"</SOLUTION>\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:19:08.301769Z","iopub.execute_input":"2025-04-13T15:19:08.302305Z","iopub.status.idle":"2025-04-13T15:19:08.305880Z","shell.execute_reply.started":"2025-04-13T15:19:08.302279Z","shell.execute_reply":"2025-04-13T15:19:08.305137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Wrap your responses to include tags\ndef wrap_response(resp):\n    return f\"{reasoning_start}Let me understand how you feel.{reasoning_end}{solution_start}{resp}{solution_end}\"\n\ndf[\"response\"] = df[\"response\"].apply(wrap_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:19:10.535266Z","iopub.execute_input":"2025-04-13T15:19:10.536008Z","iopub.status.idle":"2025-04-13T15:19:10.576333Z","shell.execute_reply.started":"2025-04-13T15:19:10.535980Z","shell.execute_reply":"2025-04-13T15:19:10.575568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"response\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:19:12.859636Z","iopub.execute_input":"2025-04-13T15:19:12.860164Z","iopub.status.idle":"2025-04-13T15:19:12.865860Z","shell.execute_reply.started":"2025-04-13T15:19:12.860140Z","shell.execute_reply":"2025-04-13T15:19:12.865036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n#Convert to HF Dataset and format for unsloth\ndataset = Dataset.from_pandas(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:19:16.144285Z","iopub.execute_input":"2025-04-13T15:19:16.144638Z","iopub.status.idle":"2025-04-13T15:19:16.234034Z","shell.execute_reply.started":"2025-04-13T15:19:16.144615Z","shell.execute_reply":"2025-04-13T15:19:16.233492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create system prompt\nsystem_prompt = f\"\"\"You are an empathetic AI and your friend. always give lovely caring message.\nUnderstand the user's feelings between {reasoning_start} and {reasoning_end}.\nThen provide a caring response between {solution_start} and {solution_end}. please give response as good friend also talk with lovely word like baby, my cutey and etc\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:08:18.139317Z","iopub.execute_input":"2025-04-13T16:08:18.139606Z","iopub.status.idle":"2025-04-13T16:08:18.143363Z","shell.execute_reply.started":"2025-04-13T16:08:18.139585Z","shell.execute_reply":"2025-04-13T16:08:18.142694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to Unsloth prompt format\ndataset = dataset.map(lambda x: {\n    \"prompt\": [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n    ],\n    \"answer\": x[\"response\"]\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:19:24.601335Z","iopub.execute_input":"2025-04-13T15:19:24.601866Z","iopub.status.idle":"2025-04-13T15:19:28.823565Z","shell.execute_reply.started":"2025-04-13T15:19:24.601840Z","shell.execute_reply":"2025-04-13T15:19:28.822971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:19:30.800848Z","iopub.execute_input":"2025-04-13T15:19:30.801114Z","iopub.status.idle":"2025-04-13T15:19:30.805791Z","shell.execute_reply.started":"2025-04-13T15:19:30.801097Z","shell.execute_reply":"2025-04-13T15:19:30.805161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset['answer'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:20:13.115693Z","iopub.execute_input":"2025-04-13T15:20:13.116211Z","iopub.status.idle":"2025-04-13T15:20:13.220921Z","shell.execute_reply.started":"2025-04-13T15:20:13.116188Z","shell.execute_reply":"2025-04-13T15:20:13.220043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n# Step 8: Define reward functions\n\nmatch_format = re.compile(\n    rf\"^[\\s]{{0,}}{reasoning_start}.+?{reasoning_end}.*?{solution_start}(.+?){solution_end}[\\s]{{0,}}$\",\n    flags=re.MULTILINE | re.DOTALL\n)\n\ndef match_format_exactly(completions, **kwargs):\n    return [1 if match_format.search(c[0][\"content\"]) else 0 for c in completions]\n\ndef match_format_approximately(completions, **kwargs):\n    scores = []\n    for completion in completions:\n        response = completion[0][\"content\"]\n        score = 0\n        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n        scores.append(score)\n    return scores\n\ndef check_answer(prompts, completions, answer, **kwargs):\n    responses = [c[0][\"content\"] for c in completions]\n    extracted = [match_format.search(r).group(1).strip() if match_format.search(r) else None for r in responses]\n    scores = []\n    for guess, true in zip(extracted, answer):\n        if not guess:\n            scores.append(0)\n        elif guess.strip() == true.strip():\n            scores.append(2.0)\n        else:\n            scores.append(1.0 if true.strip().lower() in guess.strip().lower() else 0.0)\n    return scores\n\nmatch_numbers = re.compile(rf\"{solution_start}.*?([\\d\\.]+)\", flags=re.MULTILINE | re.DOTALL)\n\ndef check_numbers(prompts, completions, answer, **kwargs):\n    responses = [c[0][\"content\"] for c in completions]\n    extracted = [match_numbers.search(r).group(1) if match_numbers.search(r) else None for r in responses]\n    scores = []\n    for guess, true in zip(extracted, answer):\n        try:\n            score = 1.5 if float(guess) == float(true) else 0\n        except:\n            score = 0\n        scores.append(score)\n    return scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:26:55.040539Z","iopub.execute_input":"2025-04-13T15:26:55.041206Z","iopub.status.idle":"2025-04-13T15:26:55.051609Z","shell.execute_reply.started":"2025-04-13T15:26:55.041180Z","shell.execute_reply":"2025-04-13T15:26:55.050850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Configure GRPO\nfrom trl import GRPOConfig, GRPOTrainer\n\nmax_prompt_length = 256\n\ntraining_args = GRPOConfig(\n    learning_rate = 5e-6,\n    adam_beta1 = 0.9,\n    adam_beta2 = 0.99,\n    weight_decay = 0.1,\n    warmup_ratio = 0.1,\n    lr_scheduler_type = \"cosine\",\n    optim = \"adamw_torch_fused\",\n    logging_steps = 1,\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 1,\n    num_generations = 4,\n    max_prompt_length = max_prompt_length,\n    max_completion_length = max_seq_length - max_prompt_length,\n    max_steps = 50,\n    save_steps = 50,\n    max_grad_norm = 0.1,\n    report_to = \"none\",\n    output_dir = \"outputs\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:27:17.081091Z","iopub.execute_input":"2025-04-13T15:27:17.081728Z","iopub.status.idle":"2025-04-13T15:27:17.117801Z","shell.execute_reply.started":"2025-04-13T15:27:17.081703Z","shell.execute_reply":"2025-04-13T15:27:17.117017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Run training\ntrainer = GRPOTrainer(\n    model = model,\n    processing_class = tokenizer,\n    reward_funcs = [\n        match_format_exactly,\n        match_format_approximately,\n        check_answer,\n        check_numbers,\n    ],\n    args = training_args,\n    train_dataset = dataset,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:27:32.875408Z","iopub.execute_input":"2025-04-13T15:27:32.875710Z","iopub.status.idle":"2025-04-13T15:58:37.741842Z","shell.execute_reply.started":"2025-04-13T15:27:32.875690Z","shell.execute_reply":"2025-04-13T15:58:37.741245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": \"I'm feeling really alone. anyone don't love for me\"}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt = True,\n    tokenize = False,\n)\n\nfrom transformers import TextStreamer\n\n_ = model.generate(\n    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n    max_new_tokens = 200,\n    temperature = 0.9,\n    top_p = 0.95,\n    top_k = 50,\n    streamer = TextStreamer(tokenizer, skip_prompt=True),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:28:53.806996Z","iopub.execute_input":"2025-04-13T16:28:53.807546Z","iopub.status.idle":"2025-04-13T16:29:12.630699Z","shell.execute_reply.started":"2025-04-13T16:28:53.807522Z","shell.execute_reply":"2025-04-13T16:29:12.630166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q huggingface_hub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:20:37.875877Z","iopub.execute_input":"2025-04-13T16:20:37.876560Z","iopub.status.idle":"2025-04-13T16:20:41.067525Z","shell.execute_reply.started":"2025-04-13T16:20:37.876538Z","shell.execute_reply":"2025-04-13T16:20:41.066656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_dir = \"empathy-chat-gemma\"\n\nmodel.save_pretrained_merged(\n    output_dir,\n    tokenizer,\n    save_method = \"merged_16bit\",   # smaller size, good for inference\n    push_to_hub = False,            # we'll push manually next\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:21:17.897581Z","iopub.execute_input":"2025-04-13T16:21:17.897889Z","iopub.status.idle":"2025-04-13T16:21:30.798696Z","shell.execute_reply.started":"2025-04-13T16:21:17.897865Z","shell.execute_reply":"2025-04-13T16:21:30.797996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from huggingface_hub import login, HfApi\n# import os\n\n# # Login if not done yet\n# login(token=\"\")\n\n# # Path to your merged model directory\n# output_dir = \"empathy-chat-gemma\"\n\n# # Push to the hub\n# api = HfApi()\n# api.upload_folder(\n#     folder_path=output_dir,\n#     repo_id=\"sajeewa/empathy-chat-gemma\",  # your repo path\n#     repo_type=\"model\"\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}